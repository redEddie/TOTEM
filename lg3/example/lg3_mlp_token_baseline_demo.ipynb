{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LG3 MLP baseline (token input) demo\n",
    "Use generalist VQ-VAE to tokenize 2-day input, then MLP predicts 1-day Power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- config ---\n",
    "ROOT = os.path.abspath(os.path.join('..', '..'))\n",
    "DATA_DIR_TRAIN = os.path.join(ROOT, 'lg3', 'data', 'processed_sources', 'elec1_f2')\n",
    "DATA_DIR_TEST = os.path.join(ROOT, 'lg3', 'data', 'processed_sources', 'ohsung_f2')\n",
    "TARGET_COL = 'Power'\n",
    "TIN = 576  # 2 days at 5-min\n",
    "TOUT = 288  # 1 day at 5-min\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "\n",
    "# generalist tokenizer checkpoint (update if needed)\n",
    "TOKENIZER_CKPT = os.path.abspath(os.path.join(\n",
    "    ROOT, 'data', 'TOTEM_data_and_pretrained_tokenizers',\n",
    "    'generatlist_pretrained_tokenizers', 'forecasting',\n",
    "    'CD64_CW256_CF4_BS4096_ITR120000', 'checkpoints', 'final_model.pth'\n",
    "))\n",
    "\n",
    "# add forecasting package to path for generalist tokenizer\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "FORECASTING_ROOT = os.path.join(ROOT, 'forecasting')\n",
    "if FORECASTING_ROOT not in sys.path:\n",
    "    sys.path.insert(0, FORECASTING_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (10157, 576) y_train (10157, 288)\n",
      "X_val (711, 576) y_val (711, 288)\n",
      "X_test (4204, 576) y_test (4204, 288)\n"
     ]
    }
   ],
   "source": [
    "def load_split(split, data_dir):\n",
    "    path = os.path.join(data_dir, f'lg3_{split}.csv')\n",
    "    df = pd.read_csv(path, parse_dates=[0], index_col=0)\n",
    "    df = df.select_dtypes(include=[np.number]).dropna(how='any')\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "    return df[[TARGET_COL]].sort_index()\n",
    "\n",
    "def build_sequences(values, seq_len, pred_len):\n",
    "    total = len(values)\n",
    "    max_start = total - (seq_len + pred_len) + 1\n",
    "    if max_start <= 0:\n",
    "        raise ValueError('Not enough rows to build sequences.')\n",
    "    x = np.empty((max_start, seq_len), dtype=np.float32)\n",
    "    y = np.empty((max_start, pred_len), dtype=np.float32)\n",
    "    for i in range(max_start):\n",
    "        x[i] = values[i : i + seq_len, 0]\n",
    "        y[i] = values[i + seq_len : i + seq_len + pred_len, 0]\n",
    "    return x, y\n",
    "\n",
    "train_df = load_split('train', DATA_DIR_TRAIN)\n",
    "val_df = load_split('val', DATA_DIR_TRAIN)\n",
    "test_df = load_split('test', DATA_DIR_TEST)\n",
    "\n",
    "X_train, y_train = build_sequences(train_df.to_numpy(np.float32), TIN, TOUT)\n",
    "X_val, y_val = build_sequences(val_df.to_numpy(np.float32), TIN, TOUT)\n",
    "X_test, y_test = build_sequences(test_df.to_numpy(np.float32), TIN, TOUT)\n",
    "\n",
    "print('X_train', X_train.shape, 'y_train', y_train.shape)\n",
    "print('X_val', X_val.shape, 'y_val', y_val.shape)\n",
    "print('X_test', X_test.shape, 'y_test', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL lib.models.vqvae.vqvae was not an allowed global by default. Please use `torch.serialization.add_safe_globals([lib.models.vqvae.vqvae])` or the `torch.serialization.safe_globals([lib.models.vqvae.vqvae])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvqvae\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m vqvae \u001b[38;5;28;01mas\u001b[39;00m gen_vqvae  \u001b[38;5;66;03m# forecasting generalist\u001b[39;00m\n\u001b[1;32m     18\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTOKENIZER_CKPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     21\u001b[0m compression_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(tokenizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompression_factor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/totme/lib/python3.10/site-packages/torch/serialization.py:1529\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1522\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1523\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1527\u001b[0m                 )\n\u001b[1;32m   1528\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1529\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1531\u001b[0m             opened_zipfile,\n\u001b[1;32m   1532\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1536\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL lib.models.vqvae.vqvae was not an allowed global by default. Please use `torch.serialization.add_safe_globals([lib.models.vqvae.vqvae])` or the `torch.serialization.safe_globals([lib.models.vqvae.vqvae])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "# min-max normalization (keeps zeros if min=0)\n",
    "x_min = X_train.min()\n",
    "x_max = X_train.max()\n",
    "x_scale = (x_max - x_min) + 1e-6\n",
    "X_train_n = (X_train - x_min) / x_scale\n",
    "X_val_n = (X_val - x_min) / x_scale\n",
    "X_test_n = (X_test - x_min) / x_scale\n",
    "\n",
    "y_min = y_train.min()\n",
    "y_max = y_train.max()\n",
    "y_scale = (y_max - y_min) + 1e-6\n",
    "y_train_n = (y_train - y_min) / y_scale\n",
    "y_val_n = (y_val - y_min) / y_scale\n",
    "y_test_n = (y_test - y_min) / y_scale\n",
    "\n",
    "from lib.models.vqvae import vqvae as gen_vqvae  # forecasting generalist\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = torch.load(TOKENIZER_CKPT, map_location=device, weights_only=False)\n",
    "tokenizer.eval()\n",
    "compression_factor = getattr(tokenizer, 'compression_factor', 4)\n",
    "\n",
    "def tokenize_batch(x_batch):\n",
    "    x_t = torch.tensor(x_batch, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        z = tokenizer.encoder(x_t, compression_factor)\n",
    "        _, _, _, _, encoding_indices, _ = tokenizer.vq(z)\n",
    "    B = x_t.shape[0]\n",
    "    L = z.shape[-1]\n",
    "    return encoding_indices.view(B, L).detach().cpu().numpy()\n",
    "\n",
    "def tokenize_all(x, batch_size=256):\n",
    "    tokens = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        tokens.append(tokenize_batch(x[i : i + batch_size]))\n",
    "    return np.concatenate(tokens, axis=0)\n",
    "\n",
    "X_train_tok = tokenize_all(X_train_n)\n",
    "X_val_tok = tokenize_all(X_val_n)\n",
    "X_test_tok = tokenize_all(X_test_n)\n",
    "\n",
    "print('X_train_tok', X_train_tok.shape)\n",
    "print('X_val_tok', X_val_tok.shape)\n",
    "print('X_test_tok', X_test_tok.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token -> embedding -> MLP\n",
    "codebook = tokenizer.vq._embedding.weight.detach().clone()\n",
    "num_codes, embed_dim = codebook.shape\n",
    "\n",
    "class TokenMLP(torch.nn.Module):\n",
    "    def __init__(self, num_codes, embed_dim, seq_len, out_dim):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(num_codes, embed_dim)\n",
    "        self.embed.weight.data.copy_(codebook)\n",
    "        self.embed.weight.requires_grad = False\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(seq_len * embed_dim, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.embed(x)\n",
    "        z = z.view(z.shape[0], -1)\n",
    "        return self.net(z)\n",
    "\n",
    "model = TokenMLP(num_codes, embed_dim, X_train_tok.shape[1], TOUT).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train_tok).long()\n",
    "y_train_t = torch.from_numpy(y_train_n).float()\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = loss_fn(pred, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'epoch {epoch+1} | train mse {total / len(train_ds):.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = torch.from_numpy(X_test_tok).long().to(device)\n",
    "    pred_n = model(X_test_t).cpu().numpy()\n",
    "\n",
    "pred = pred_n * y_scale + y_min\n",
    "mse = np.mean((y_test - pred) ** 2)\n",
    "mae = np.mean(np.abs(y_test - pred))\n",
    "print(f'Test MSE: {mse:.6f} | MAE: {mae:.6f}')\n",
    "\n",
    "# plot one example with input context\n",
    "idx = int(0.2 * X_test.shape[0])\n",
    "inp = X_test[idx]\n",
    "true = y_test[idx]\n",
    "pred_y = pred[idx]\n",
    "\n",
    "full_x = np.concatenate([inp, true])\n",
    "full_pred = np.concatenate([inp, pred_y])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(full_x, label='true (input+target)', linewidth=1.0)\n",
    "plt.plot(full_pred, label='pred (input+forecast)', linewidth=1.0)\n",
    "plt.axvline(TIN - 1, color='k', linestyle='--', linewidth=0.8)\n",
    "plt.legend()\n",
    "plt.title('Token MLP: input context + 1-day forecast')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "totme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
